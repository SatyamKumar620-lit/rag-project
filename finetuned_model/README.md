---
tags:
- sentence-transformers
- sentence-similarity
- feature-extraction
- generated_from_trainer
- dataset_size:2508
- loss:MultipleNegativesRankingLoss
base_model: sentence-transformers/all-MiniLM-L6-v2
widget:
- source_sentence: ‚ÄúTo die will be an awfully big adventure.‚Äù - J.M. Barrie,
  sentences:
  - ‚ÄúChance is perhaps the pseudonym of God when he does not want to sign.‚Äù - Th√É¬©ophile
    Gautier
  - ‚ÄúWhen I despair, I remember that all through history the way of truth and love
    have always won. There have been tyrants and murderers, and for a time, they can
    seem invincible, but in the end, they always fall. Think of it--always.‚Äù - Mahatma
    Gandhi
  - ‚ÄúTo die will be an awfully big adventure.‚Äù - J.M. Barrie,
- source_sentence: ‚ÄúIf I should have a daughter√¢‚Ç¨¬¶√¢‚Ç¨≈ìInstead of √¢‚Ç¨≈ìMom√¢‚Ç¨ÔøΩ, she√¢‚Ç¨‚Ñ¢s
    gonna call me √¢‚Ç¨≈ìPoint B.√¢‚Ç¨ÔøΩ Because that way, she knows that no matter what happens,
    at least she can always find her way to me. And I√¢‚Ç¨‚Ñ¢m going to paint the solar
    system on the back of her hands so that she has to learn the entire universe before
    she can say √¢‚Ç¨≈ìOh, I know that like the back of my hand.√¢‚Ç¨ÔøΩShe√¢‚Ç¨‚Ñ¢s gonna learn
    that this life will hit you, hard, in the face, wait for you to get back up so
    it can kick you in the stomach. But getting the wind knocked out of you is the
    only way to remind your lungs how much they like the taste of air. There is hurt,
    here, that cannot be fixed by band-aids or poetry, so the first time she realizes
    that Wonder-woman isn√¢‚Ç¨‚Ñ¢t coming, I√¢‚Ç¨‚Ñ¢ll make sure she knows she doesn√¢‚Ç¨‚Ñ¢t have
    to wear the cape all by herself. Because no matter how wide you stretch your fingers,
    your hands will always be too small to catch all the pain you want to heal. Believe
    me, I√¢‚Ç¨‚Ñ¢ve tried.And √¢‚Ç¨≈ìBaby,√¢‚Ç¨ÔøΩ I√¢‚Ç¨‚Ñ¢ll tell her √¢‚Ç¨≈ìdon√¢‚Ç¨‚Ñ¢t keep your nose up
    in the air like that, I know that trick, you√¢‚Ç¨‚Ñ¢re just smelling for smoke so you
    can follow the trail back to a burning house so you can find the boy who lost
    everything in the fire to see if you can save him. Or else, find the boy who lit
    the fire in the first place to see if you can change him.√¢‚Ç¨ÔøΩBut I know that she
    will anyway, so instead I√¢‚Ç¨‚Ñ¢ll always keep an extra supply of chocolate and rain
    boats nearby, √¢‚Ç¨Àúcause there is no heartbreak that chocolate can√¢‚Ç¨‚Ñ¢t fix. Okay,
    there√¢‚Ç¨‚Ñ¢s a few heartbreaks chocolate can√¢‚Ç¨‚Ñ¢t fix. But that√¢‚Ç¨‚Ñ¢s what the rain
    boots are for, because rain will wash away everything if you let it.I want her
    to see the world through the underside of a glass bottom boat, to look through
    a magnifying glass at the galaxies that exist on the pin point of a human mind.
    Because that√¢‚Ç¨‚Ñ¢s how my mom taught me. That there√¢‚Ç¨‚Ñ¢ll be days like this, √¢‚Ç¨≈ìThere√¢‚Ç¨‚Ñ¢ll
    be days like this my momma said√¢‚Ç¨ÔøΩ when you open your hands to catch and wind
    up with only blisters and bruises. When you step out of the phone booth and try
    to fly and the very people you wanna save are the ones standing on your cape.
    When your boots will fill with rain and you√¢‚Ç¨‚Ñ¢ll be up to your knees in disappointment
    and those are the very days you have all the more reason to say √¢‚Ç¨≈ìthank you,√¢‚Ç¨ÔøΩ
    √¢‚Ç¨Àúcause there is nothing more beautiful than the way the ocean refuses to stop
    kissing the shoreline no matter how many times it√¢‚Ç¨‚Ñ¢s sent away.You will put the
    √¢‚Ç¨≈ìwind√¢‚Ç¨ÔøΩ in win some lose some, you will put the √¢‚Ç¨≈ìstar√¢‚Ç¨ÔøΩ in starting over
    and over, and no matter how many land mines erupt in a minute be sure your mind
    lands on the beauty of this funny place called life.And yes, on a scale from one
    to over-trusting I am pretty damn naive but I want her to know that this world
    is made out of sugar. It can crumble so easily but don√¢‚Ç¨‚Ñ¢t be afraid to stick
    your tongue out and taste it.√¢‚Ç¨≈ìBaby,√¢‚Ç¨ÔøΩ I√¢‚Ç¨‚Ñ¢ll tell her √¢‚Ç¨≈ìremember your mama
    is a worrier but your papa is a warrior and you are the girl with small hands
    and big eyes who never stops asking for more.√¢‚Ç¨ÔøΩRemember that good things come
    in threes and so do bad things and always apologize when you√¢‚Ç¨‚Ñ¢ve done something
    wrong but don√¢‚Ç¨‚Ñ¢t you ever apologize for the way your eyes refuse to stop shining.Your
    voice is small but don√¢‚Ç¨‚Ñ¢t ever stop singing and when they finally hand you heartbreak,
    slip hatred and war under your doorstep and hand you hand-outs on street corners
    of cynicism and defeat, you tell them that they really ought to meet your mother.‚Äù
    - Sarah Kay
  sentences:
  - ‚ÄúIf I should have a daughter√¢‚Ç¨¬¶√¢‚Ç¨≈ìInstead of √¢‚Ç¨≈ìMom√¢‚Ç¨ÔøΩ, she√¢‚Ç¨‚Ñ¢s gonna call me
    √¢‚Ç¨≈ìPoint B.√¢‚Ç¨ÔøΩ Because that way, she knows that no matter what happens, at least
    she can always find her way to me. And I√¢‚Ç¨‚Ñ¢m going to paint the solar system on
    the back of her hands so that she has to learn the entire universe before she
    can say √¢‚Ç¨≈ìOh, I know that like the back of my hand.√¢‚Ç¨ÔøΩShe√¢‚Ç¨‚Ñ¢s gonna learn that
    this life will hit you, hard, in the face, wait for you to get back up so it can
    kick you in the stomach. But getting the wind knocked out of you is the only way
    to remind your lungs how much they like the taste of air. There is hurt, here,
    that cannot be fixed by band-aids or poetry, so the first time she realizes that
    Wonder-woman isn√¢‚Ç¨‚Ñ¢t coming, I√¢‚Ç¨‚Ñ¢ll make sure she knows she doesn√¢‚Ç¨‚Ñ¢t have to
    wear the cape all by herself. Because no matter how wide you stretch your fingers,
    your hands will always be too small to catch all the pain you want to heal. Believe
    me, I√¢‚Ç¨‚Ñ¢ve tried.And √¢‚Ç¨≈ìBaby,√¢‚Ç¨ÔøΩ I√¢‚Ç¨‚Ñ¢ll tell her √¢‚Ç¨≈ìdon√¢‚Ç¨‚Ñ¢t keep your nose up
    in the air like that, I know that trick, you√¢‚Ç¨‚Ñ¢re just smelling for smoke so you
    can follow the trail back to a burning house so you can find the boy who lost
    everything in the fire to see if you can save him. Or else, find the boy who lit
    the fire in the first place to see if you can change him.√¢‚Ç¨ÔøΩBut I know that she
    will anyway, so instead I√¢‚Ç¨‚Ñ¢ll always keep an extra supply of chocolate and rain
    boats nearby, √¢‚Ç¨Àúcause there is no heartbreak that chocolate can√¢‚Ç¨‚Ñ¢t fix. Okay,
    there√¢‚Ç¨‚Ñ¢s a few heartbreaks chocolate can√¢‚Ç¨‚Ñ¢t fix. But that√¢‚Ç¨‚Ñ¢s what the rain
    boots are for, because rain will wash away everything if you let it.I want her
    to see the world through the underside of a glass bottom boat, to look through
    a magnifying glass at the galaxies that exist on the pin point of a human mind.
    Because that√¢‚Ç¨‚Ñ¢s how my mom taught me. That there√¢‚Ç¨‚Ñ¢ll be days like this, √¢‚Ç¨≈ìThere√¢‚Ç¨‚Ñ¢ll
    be days like this my momma said√¢‚Ç¨ÔøΩ when you open your hands to catch and wind
    up with only blisters and bruises. When you step out of the phone booth and try
    to fly and the very people you wanna save are the ones standing on your cape.
    When your boots will fill with rain and you√¢‚Ç¨‚Ñ¢ll be up to your knees in disappointment
    and those are the very days you have all the more reason to say √¢‚Ç¨≈ìthank you,√¢‚Ç¨ÔøΩ
    √¢‚Ç¨Àúcause there is nothing more beautiful than the way the ocean refuses to stop
    kissing the shoreline no matter how many times it√¢‚Ç¨‚Ñ¢s sent away.You will put the
    √¢‚Ç¨≈ìwind√¢‚Ç¨ÔøΩ in win some lose some, you will put the √¢‚Ç¨≈ìstar√¢‚Ç¨ÔøΩ in starting over
    and over, and no matter how many land mines erupt in a minute be sure your mind
    lands on the beauty of this funny place called life.And yes, on a scale from one
    to over-trusting I am pretty damn naive but I want her to know that this world
    is made out of sugar. It can crumble so easily but don√¢‚Ç¨‚Ñ¢t be afraid to stick
    your tongue out and taste it.√¢‚Ç¨≈ìBaby,√¢‚Ç¨ÔøΩ I√¢‚Ç¨‚Ñ¢ll tell her √¢‚Ç¨≈ìremember your mama
    is a worrier but your papa is a warrior and you are the girl with small hands
    and big eyes who never stops asking for more.√¢‚Ç¨ÔøΩRemember that good things come
    in threes and so do bad things and always apologize when you√¢‚Ç¨‚Ñ¢ve done something
    wrong but don√¢‚Ç¨‚Ñ¢t you ever apologize for the way your eyes refuse to stop shining.Your
    voice is small but don√¢‚Ç¨‚Ñ¢t ever stop singing and when they finally hand you heartbreak,
    slip hatred and war under your doorstep and hand you hand-outs on street corners
    of cynicism and defeat, you tell them that they really ought to meet your mother.‚Äù
    - Sarah Kay
  - ‚ÄúAs usual, there is a great woman behind every idiot.‚Äù - John Lennon
  - ‚ÄúHearts are made to be broken.‚Äù - Oscar Wilde,
- source_sentence: ‚ÄúSometimes, you read a book and it fills you with this weird evangelical
    zeal, and you become convinced that the shattered world will never be put back
    together unless and until all living humans read the book. And then there are
    books like An Imperial Affliction, which you can't tell people about, books so
    special and rare and yours that advertising your affection feels like betrayal‚Äù
    - John Green,
  sentences:
  - '‚ÄúThe boy never cried again, and he never forgot what he''d learned: that to love
    is to destroy, and that to be loved is to be the one destroyed.‚Äù - Cassandra Clare,'
  - ‚ÄúSometimes, you read a book and it fills you with this weird evangelical zeal,
    and you become convinced that the shattered world will never be put back together
    unless and until all living humans read the book. And then there are books like
    An Imperial Affliction, which you can't tell people about, books so special and
    rare and yours that advertising your affection feels like betrayal‚Äù - John Green,
  - ‚ÄúIn heaven, all the interesting people are missing.‚Äù - Friedrich Nietzsche
- source_sentence: ‚ÄúThat's what careless words do. They make people love you a little
    less.‚Äù - Arundhati Roy,
  sentences:
  - ‚ÄúLaughter is wine for the soul - laughter soft, or loud and deep, tinged through
    with seriousness - the hilarious declaration made by man that life is worth living.‚Äù
    - Sean O'Casey
  - ‚ÄúHope can be a powerful force. Maybe there's no actual magic in it, but when you
    know what you hope for most and hold it like a light within you, you can make
    things happen, almost like magic.‚Äù - Laini Taylor,
  - ‚ÄúThat's what careless words do. They make people love you a little less.‚Äù - Arundhati
    Roy,
- source_sentence: ‚ÄúYou do not write your life with words...You write it with actions.
    What you think is not important. It is only important what you do.‚Äù - Patrick
    Ness,
  sentences:
  - ‚ÄúLive to the point of tears.‚Äù - Albert Camus
  - ‚ÄúGrowing apart doesn't change the fact that for a long time we grew side by side;
    our roots will always be tangled. I'm glad for that.‚Äù - Ally Condie,
  - ‚ÄúYou do not write your life with words...You write it with actions. What you think
    is not important. It is only important what you do.‚Äù - Patrick Ness,
pipeline_tag: sentence-similarity
library_name: sentence-transformers
---

# SentenceTransformer based on sentence-transformers/all-MiniLM-L6-v2

This is a [sentence-transformers](https://www.SBERT.net) model finetuned from [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2). It maps sentences & paragraphs to a 384-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

### Model Description
- **Model Type:** Sentence Transformer
- **Base model:** [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) <!-- at revision c9745ed1d9f207416be6d2e6f8de32d1f16199bf -->
- **Maximum Sequence Length:** 512 tokens
- **Output Dimensionality:** 384 dimensions
- **Similarity Function:** Cosine Similarity
<!-- - **Training Dataset:** Unknown -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->

### Model Sources

- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)

### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
)
```

## Usage

### Direct Usage (Sentence Transformers)

First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the ü§ó Hub
model = SentenceTransformer("sentence_transformers_model_id")
# Run inference
sentences = [
    '‚ÄúYou do not write your life with words...You write it with actions. What you think is not important. It is only important what you do.‚Äù - Patrick Ness,',
    '‚ÄúYou do not write your life with words...You write it with actions. What you think is not important. It is only important what you do.‚Äù - Patrick Ness,',
    '‚ÄúLive to the point of tears.‚Äù - Albert Camus',
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 384]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [3, 3]
```

<!--
### Direct Usage (Transformers)

<details><summary>Click to see the direct usage in Transformers</summary>

</details>
-->

<!--
### Downstream Usage (Sentence Transformers)

You can finetune this model on your own dataset.

<details><summary>Click to expand</summary>

</details>
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Dataset

#### Unnamed Dataset

* Size: 2,508 training samples
* Columns: <code>sentence_0</code> and <code>sentence_1</code>
* Approximate statistics based on the first 1000 samples:
  |         | sentence_0                                                                         | sentence_1                                                                         |
  |:--------|:-----------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------|
  | type    | string                                                                             | string                                                                             |
  | details | <ul><li>min: 9 tokens</li><li>mean: 48.37 tokens</li><li>max: 512 tokens</li></ul> | <ul><li>min: 9 tokens</li><li>mean: 48.37 tokens</li><li>max: 512 tokens</li></ul> |
* Samples:
  | sentence_0                                                                                                                                                      | sentence_1                                                                                                                                                      |
  |:----------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------|
  | <code>‚ÄúThere are three rules for writing a novel. Unfortunately, no one knows what they are.‚Äù - W. Somerset Maugham</code>                                      | <code>‚ÄúThere are three rules for writing a novel. Unfortunately, no one knows what they are.‚Äù - W. Somerset Maugham</code>                                      |
  | <code>‚ÄúI figured all your classes were stuff like Slaughter 101 and Beheading for Beginners."Jace flipped a page. "Very funny, Fray.‚Äù - Cassandra Clare,</code> | <code>‚ÄúI figured all your classes were stuff like Slaughter 101 and Beheading for Beginners."Jace flipped a page. "Very funny, Fray.‚Äù - Cassandra Clare,</code> |
  | <code>‚ÄúAnd so it goes...‚Äù - Kurt Vonnegut,</code>                                                                                                               | <code>‚ÄúAnd so it goes...‚Äù - Kurt Vonnegut,</code>                                                                                                               |
* Loss: [<code>MultipleNegativesRankingLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss) with these parameters:
  ```json
  {
      "scale": 20.0,
      "similarity_fct": "cos_sim"
  }
  ```

### Training Hyperparameters
#### Non-Default Hyperparameters

- `num_train_epochs`: 1
- `multi_dataset_batch_sampler`: round_robin

#### All Hyperparameters
<details><summary>Click to expand</summary>

- `overwrite_output_dir`: False
- `do_predict`: False
- `eval_strategy`: no
- `prediction_loss_only`: True
- `per_device_train_batch_size`: 8
- `per_device_eval_batch_size`: 8
- `per_gpu_train_batch_size`: None
- `per_gpu_eval_batch_size`: None
- `gradient_accumulation_steps`: 1
- `eval_accumulation_steps`: None
- `torch_empty_cache_steps`: None
- `learning_rate`: 5e-05
- `weight_decay`: 0.0
- `adam_beta1`: 0.9
- `adam_beta2`: 0.999
- `adam_epsilon`: 1e-08
- `max_grad_norm`: 1
- `num_train_epochs`: 1
- `max_steps`: -1
- `lr_scheduler_type`: linear
- `lr_scheduler_kwargs`: {}
- `warmup_ratio`: 0.0
- `warmup_steps`: 0
- `log_level`: passive
- `log_level_replica`: warning
- `log_on_each_node`: True
- `logging_nan_inf_filter`: True
- `save_safetensors`: True
- `save_on_each_node`: False
- `save_only_model`: False
- `restore_callback_states_from_checkpoint`: False
- `no_cuda`: False
- `use_cpu`: False
- `use_mps_device`: False
- `seed`: 42
- `data_seed`: None
- `jit_mode_eval`: False
- `use_ipex`: False
- `bf16`: False
- `fp16`: False
- `fp16_opt_level`: O1
- `half_precision_backend`: auto
- `bf16_full_eval`: False
- `fp16_full_eval`: False
- `tf32`: None
- `local_rank`: 0
- `ddp_backend`: None
- `tpu_num_cores`: None
- `tpu_metrics_debug`: False
- `debug`: []
- `dataloader_drop_last`: False
- `dataloader_num_workers`: 0
- `dataloader_prefetch_factor`: None
- `past_index`: -1
- `disable_tqdm`: False
- `remove_unused_columns`: True
- `label_names`: None
- `load_best_model_at_end`: False
- `ignore_data_skip`: False
- `fsdp`: []
- `fsdp_min_num_params`: 0
- `fsdp_config`: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
- `fsdp_transformer_layer_cls_to_wrap`: None
- `accelerator_config`: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
- `deepspeed`: None
- `label_smoothing_factor`: 0.0
- `optim`: adamw_torch
- `optim_args`: None
- `adafactor`: False
- `group_by_length`: False
- `length_column_name`: length
- `ddp_find_unused_parameters`: None
- `ddp_bucket_cap_mb`: None
- `ddp_broadcast_buffers`: False
- `dataloader_pin_memory`: True
- `dataloader_persistent_workers`: False
- `skip_memory_metrics`: True
- `use_legacy_prediction_loop`: False
- `push_to_hub`: False
- `resume_from_checkpoint`: None
- `hub_model_id`: None
- `hub_strategy`: every_save
- `hub_private_repo`: None
- `hub_always_push`: False
- `gradient_checkpointing`: False
- `gradient_checkpointing_kwargs`: None
- `include_inputs_for_metrics`: False
- `include_for_metrics`: []
- `eval_do_concat_batches`: True
- `fp16_backend`: auto
- `push_to_hub_model_id`: None
- `push_to_hub_organization`: None
- `mp_parameters`: 
- `auto_find_batch_size`: False
- `full_determinism`: False
- `torchdynamo`: None
- `ray_scope`: last
- `ddp_timeout`: 1800
- `torch_compile`: False
- `torch_compile_backend`: None
- `torch_compile_mode`: None
- `include_tokens_per_second`: False
- `include_num_input_tokens_seen`: False
- `neftune_noise_alpha`: None
- `optim_target_modules`: None
- `batch_eval_metrics`: False
- `eval_on_start`: False
- `use_liger_kernel`: False
- `eval_use_gather_object`: False
- `average_tokens_across_devices`: False
- `prompts`: None
- `batch_sampler`: batch_sampler
- `multi_dataset_batch_sampler`: round_robin

</details>

### Framework Versions
- Python: 3.11.3
- Sentence Transformers: 4.1.0
- Transformers: 4.52.4
- PyTorch: 2.7.1+cpu
- Accelerate: 1.8.1
- Datasets: 3.6.0
- Tokenizers: 0.21.2

## Citation

### BibTeX

#### Sentence Transformers
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```

#### MultipleNegativesRankingLoss
```bibtex
@misc{henderson2017efficient,
    title={Efficient Natural Language Response Suggestion for Smart Reply},
    author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},
    year={2017},
    eprint={1705.00652},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->